{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle as pkl\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import math\n",
    "import colorsys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def resize_to_3d(image: np.array):\n",
    "    img = np.reshape(image, (32, 32, 1))\n",
    "    img = np.repeat(img, 3, axis=3)\n",
    "    return img\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "def kmeans( image, n ):                     # Uses k-Means to quantize an image into n colors.\n",
    "    image = cv2.cvtColor( image, cv2.COLOR_BGR2LAB )# The LAB color space is especially useful for this type of system, as it's similar to how a human eye works\n",
    "    w, h, _ = image.shape                           # Gets the shape of the image for the next step\n",
    "    image = image.reshape( ( image.shape[ 0 ] * image.shape[ 1 ], 3 ) ) #k-Means required an image to be one pixel tall for some reason\n",
    "    clt = MiniBatchKMeans( n_clusters = n )         # Creates a new kMeans system. Cluster count tells how many regions of best fit should be made.\n",
    "    labels = clt.fit_predict( image )               # This takes a aet of inputted colors and tries to find clusters of data. Similar to how one might move a '3d cursor' to the center of data on -a 3d mqp\n",
    "    quant = clt.cluster_centers_.astype( \"uint8\" )[ labels ]    #Replaces every pixel with the closewt color k-Means found\n",
    "    quant = quant.reshape( ( h, w, 3 ) )            # Rescales the quantized image back into its original shape. Simply imagine the pixels line wrqpping similar to a wore processor\n",
    "    quant = cv2.cvtColor( quant, cv2.COLOR_LAB2BGR )# Reverts the color as well\n",
    "    #for i in labels: print(labels)\n",
    "    return quant, labels\n",
    "\n",
    "\n",
    "def preprocess(x_train, x_test, y_train, y_test, num_classes, regression=False):\n",
    "    if type(x_train) != np.ndarray:\n",
    "        x_train = x_train.to_numpy().astype('float32')\n",
    "        x_test = x_test.to_numpy().astype('float32')\n",
    "        y_train = y_train.to_numpy().astype('float32')\n",
    "        y_test = y_test.to_numpy().astype('float32')\n",
    "    if regression == False:\n",
    "        y_train = keras.utils.to_categorical(y_train, num_classes=num_classes)\n",
    "        y_test = keras.utils.to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "    x_train = np.reshape(x_train, (-1, int(math.sqrt(x_train.shape[1])), int(math.sqrt(x_train.shape[1]))))\n",
    "    x_test = np.reshape(x_test, (-1, int(math.sqrt(x_test.shape[1])), int(math.sqrt(x_test.shape[1]))))\n",
    "\n",
    "    x_train_ = np.zeros((x_train.shape[0], 32, 32))\n",
    "    x_test_ = np.zeros((x_test.shape[0], 32, 32))\n",
    "\n",
    "    for i in range(x_train.shape[0]):\n",
    "        x_train_[i] = cv2.resize(x_train[i], (32, 32))\n",
    "\n",
    "    for i in range(x_test.shape[0]):\n",
    "        x_test_[i] = cv2.resize(x_test[i], (32, 32))\n",
    "\n",
    "    x_train = x_train_\n",
    "    x_test = x_test_\n",
    "\n",
    "    x_train = np.reshape(x_train, (-1, 32, 32, 1))\n",
    "    x_test = np.reshape(x_test, (-1, 32, 32, 1))\n",
    "\n",
    "    x_train = np.repeat(x_train, 3, axis=3)\n",
    "    x_test = np.repeat(x_test, 3, axis=3)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def avg_distance_from_center(image: np.array):\n",
    "    subs = (np.argwhere(image>0) - np.array([5/2, 5/2]))\n",
    "    out = np.sqrt(np.einsum('ij,ij->i',subs,subs)).mean()\n",
    "    return out\n",
    "\n",
    "def view(image):\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"image\", \"shape\", \"letter\", \"rotation\"])\n",
    "df[\"image\"] = df[\"image\"].astype(object)\n",
    "\n",
    "shape_labels = [\"circle\", \"semicircle\", \"quartercircle\", \"triangle\",\n",
    "          \"square\", \"rectangle\", \"trapezoid\", \"pentagon\", \"hexagon\",\n",
    "          \"heptagon\", \"octagon\", \"star\", \"cross\"]\n",
    "\n",
    "with open('./data/vals.txt') as f:\n",
    "    for line in f.readlines():\n",
    "        path, label = line.split(' ')\n",
    "        label = [int(i) for i in label.split(\",\")]\n",
    "        x1, y1, x2, y2, zero = label\n",
    "        img = cv2.imread(f\"./data/{path}\")\n",
    "        img = img[y1:y2, x1:x2]\n",
    "        size=28  # 28x28 image\n",
    "        #dx = size-(x2-x1)\n",
    "        #dy = size-(y2-y1)\n",
    "        #if dx % 2 == 1:  # if odd, do put +1 more padding on one side\n",
    "        #    fx1 = (dx-1)//2  # x dim first padding\n",
    "        #    fx2 = fx1+1  # x dim last padding\n",
    "#\n",
    "        #    fy1 = (dy-1)//2  # y dim first padding\n",
    "        #    fy2 = fy1+1  # y dim last padding\n",
    "        #else:\n",
    "        #    fx1, fx2 = dx//2, dx//2\n",
    "        #    fy1, fy2 = dy//2, dy//2\n",
    "#\n",
    "        #padded_img = np.pad(img, ((fy1, fy2), (fx1, fx2), (0, 0)))\n",
    "        padded_img = cv2.resize(padded_img, (32, 32))\n",
    "        nothing, shape, letter, rotation = path.split(\"_\")\n",
    "        rotation = rotation.split(\".\")[0]\n",
    "        data = {\n",
    "            \"image\": [padded_img],\n",
    "            \"shape\": [shape],\n",
    "            \"letter\": [letter],\n",
    "            \"rotation\": [rotation]\n",
    "        }\n",
    "        df2 = pd.DataFrame(data)\n",
    "        df = df.append(df2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Letter Classifier\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "images = df[\"image\"].to_numpy()\n",
    "images = np.array(images)\n",
    "images = np.vstack(images)\n",
    "images = np.reshape(images, (-1, 75, 75, 3))\n",
    "\n",
    "labels = np.asarray(df[\"letter\"])\n",
    "encode = np.vectorize(alphabet.index)\n",
    "labels = encode(labels)\n",
    "labels = keras.utils.to_categorical(labels, num_classes=26)\n",
    "\n",
    "images_ = np.zeros((images.shape[0], 32, 32))\n",
    "\n",
    "for i in range(images.shape[0]):\n",
    "    images_[i] = cv2.resize(images[i], (32, 32))\n",
    "\n",
    "images = images_\n",
    "\n",
    "#data_augmentation = keras.Sequential(\n",
    "#    [\n",
    "#        keras.layers.RandomFlip(),\n",
    "#        keras.layers.RandomRotation(0.1),\n",
    "#        keras.layers.RandomZoom(0.1),\n",
    "#    ]\n",
    "#)\n",
    "#\n",
    "#train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "#train_dataset = train_dataset.batch(16).map(lambda x, y: (data_augmentation(x), y))\n",
    "#\n",
    "#test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "#train_dataset = test_dataset.batch(16)\n",
    "model = keras.models.load_model('./models/letter_classifier')\n",
    "y = model.predict(images[0])\n",
    "print(y)\n",
    "#model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "             # loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "#callback = keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", min_delta=.5, patience=3, restore_best_weights=True)\n",
    "\n",
    "#model.fit(x=x_train, y=y_train, batch_size=16, epochs=100, validation_data=(x_test, y_test), shuffle=True)#, callbacks=[callback])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "df_letter = pd.read_csv(\"letter_data.csv\", names=[str(i) for i in range(785)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class ConvNetLetter(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ConvNetLetter, self).__init__()\n",
    "        self.norm = tf.keras.layers.Normalization(axis=None)\n",
    "        self.resnet = keras.applications.ResNet50V2(weights='imagenet', include_top=False,\n",
    "                                                    input_shape=(32, 32, 3))\n",
    "        for layer in self.resnet.layers:\n",
    "            layer.trainable = False\n",
    "        self.model = keras.Sequential([\n",
    "            keras.layers.Flatten(),\n",
    "            keras.layers.Dense(26, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.norm(inputs)\n",
    "        x = self.resnet(x)\n",
    "        return self.model(x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(312858, 32, 32, 3)\n",
      "(312858, 26)\n",
      "Epoch 1/100\n",
      "4889/4889 [==============================] - 76s 15ms/step - loss: 7.3532 - accuracy: 0.7508 - val_loss: 10.2017 - val_accuracy: 0.7545\n",
      "Epoch 2/100\n",
      "4889/4889 [==============================] - 72s 15ms/step - loss: 5.0534 - accuracy: 0.8376 - val_loss: 5.8809 - val_accuracy: 0.8091\n",
      "Epoch 3/100\n",
      "4889/4889 [==============================] - 72s 15ms/step - loss: 4.7527 - accuracy: 0.8559 - val_loss: 6.1472 - val_accuracy: 0.8218\n",
      "Epoch 4/100\n",
      "4889/4889 [==============================] - 71s 14ms/step - loss: 4.5664 - accuracy: 0.8653 - val_loss: 4.2156 - val_accuracy: 0.8866\n",
      "Epoch 5/100\n",
      "4889/4889 [==============================] - 71s 14ms/step - loss: 4.4711 - accuracy: 0.8719 - val_loss: 12.0749 - val_accuracy: 0.7271\n",
      "Epoch 6/100\n",
      "4889/4889 [==============================] - 70s 14ms/step - loss: 4.4212 - accuracy: 0.8775 - val_loss: 6.8827 - val_accuracy: 0.8127\n",
      "INFO:tensorflow:Assets written to: ./models/letter_classifier\\assets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_letter.drop(columns=\"0\"), df_letter[\"0\"], test_size=0.16,\n",
    "                                                    random_state=19, stratify=df_letter[\"0\"])\n",
    "\n",
    "x_train, x_test, y_train, y_test = preprocess(x_train, x_test, y_train, y_test, 26)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "model = ConvNetLetter()\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "callback = keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", min_delta=.5, patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit(x=x_train, y=y_train, batch_size=64, epochs=100, validation_data=(x_test, y_test), shuffle=True, callbacks=[callback])\n",
    "model.save(\"./models/letter_classifier\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9604,)\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'wefwefwe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [80]\u001B[0m, in \u001B[0;36m<cell line: 9>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      7\u001B[0m plt\u001B[38;5;241m.\u001B[39mimshow(x)\n\u001B[0;32m      8\u001B[0m plt\u001B[38;5;241m.\u001B[39mshow()\n\u001B[1;32m----> 9\u001B[0m \u001B[43mwefwefwe\u001B[49m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'wefwefwe' is not defined"
     ]
    }
   ],
   "source": [
    "df_shape = pd.read_csv(\"shape_data2.csv\", names=[str(i) for i in range(98*98 + 1)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ConvNetShapeLetter(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ConvNetShapeLetter, self).__init__()\n",
    "        self.norm = tf.keras.layers.Normalization(axis=None)\n",
    "        self.resnet = keras.applications.ResNet50V2(weights='imagenet', include_top=False,\n",
    "                                                    input_shape=(32, 32, 3))\n",
    "        for layer in self.resnet.layers:\n",
    "            layer.trainable = False\n",
    "        self.model = keras.Sequential([\n",
    "            keras.layers.Flatten(),\n",
    "            keras.layers.Dense(13, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.norm(inputs)\n",
    "        x = self.resnet(x)\n",
    "        return self.model(x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10920, 32, 32, 3)\n",
      "(10920, 13)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAMpElEQVR4nO3dX6hl5X3G8e9THdsShWhth2E0NVppCSFVEUlhCDaQYOdGhSIGChYCJ5QKelGopNDYXiUlGnplmVaJlNbU1qaKlBorFnNlHO04jk4TNShxGB2CFfUmqfHXi72Gnpme/Wf2Xmvvs8/7/cDmrL3OPmv95p3z7Hetd+2z3lQVkna+n1t1AZKWw7BLjTDsUiMMu9QIwy41wrBLjTh7kR9Och3wl8BZwN9U1VenvN7rfNLAqipbrc+819mTnAX8APgc8AbwDPCFqnppws8Ydmlg48K+yGH8NcArVfXDqvop8C3g+gW2J2lAi4R9L/CjTc/f6NZJ2oYWOmefRZINYGPo/UiabJGwHwMu3vT8om7dKarqAHAAPGeXVmmRw/hngMuTfDzJOcDNwCP9lCWpb3P37FX1QZJbgccYXXq7r6pe7K0ySb2a+9LbXDvzMF4a3BCX3iStEcMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUiIVmcU3yGvAe8DPgg6q6uo+iJPWvjymbf7uqftzDdiQNyMN4qRGLhr2A7yR5NslGHwVJGsaih/H7qupYkl8BHk/yX1X11OYXdG8CvhFIK9bblM1J7gTer6qvT3iNUzZLA+t9yuYkH0ly3sll4PPAkXm3J2lYixzG7wa+neTkdv6+qv6tl6rUpHmPMrvfQU3R22H8TDvzMF4TGPZ+9H4YL2m9GHapEYZdaoRhlxph2KVG9PGHMNLMhrj6M2mbjtT/H3t2qRGGXWqEYZcaYdilRhh2qRGOxp+BvkeSd/JI8TL/5mKScXXs5LYfx55daoRhlxph2KVGGHapEYZdaoRhlxrhpbfTLPOS0brfhmm7XF7TbOzZpUYYdqkRhl1qhGGXGmHYpUYYdqkRU8Oe5L4kJ5Ic2bTugiSPJ3m5+3r+sGVqs6oa+1jnfWlYs/Ts3wSuO23dHcATVXU58ET3XNI2NjXs3Xzrb5+2+nrg/m75fuCGfsuS1Ld5z9l3V9XxbvlNRjO6StrGFv64bFXVpNlZk2wAG4vuR9Ji5u3Z30qyB6D7emLcC6vqQFVdXVVXz7kvST2YN+yPALd0y7cAD/dTjqShZNollCQPANcCFwJvAV8B/gV4EPgY8DpwU1WdPoi31bbW+nrNOl9umvcv5db53wzb5y8El6mqtvxHTw17nwz76hj2dowLu5+gkxph2KVGGHapEYZdaoRhlxrhDSfPwLiR3XUYsV6HGufV4oj7POzZpUYYdqkRhl1qhGGXGmHYpUYYdqkRXnrrwaRLPzv5ktcyeXltcfbsUiMMu9QIwy41wrBLjTDsUiMcjR+YI/Wzc8R9WPbsUiMMu9QIwy41wrBLjTDsUiMMu9SIqWFPcl+SE0mObFp3Z5JjSQ51j/3DlrkzJRn7kPo2S8/+TeC6LdZ/o6qu6B7/2m9Zkvo2NexV9RQwddJGSdvbIufstyY53B3mn99bRZIGMW/Y7wEuA64AjgN3jXthko0kB5McnHNfknow05TNSS4BHq2qT57J97Z4rR8Gn1GLn5t3YLIfvU7ZnGTPpqc3AkfGvVbS9jD1r96SPABcC1yY5A3gK8C1Sa4ACngN+NJwJWonsfdenZkO43vbmYfxM9uph/GGfXi9HsZLWj+GXWqEYZcaYdilRhh2qRHecHKFduqI+yST/s2O1A/Lnl1qhGGXGmHYpUYYdqkRhl1qhGGXGuGlt4G1eHltXl6WG5Y9u9QIwy41wrBLjTDsUiMMu9QIR+N74Ij78BypX5w9u9QIwy41wrBLjTDsUiMMu9QIwy41YmrYk1yc5MkkLyV5Mclt3foLkjye5OXu646ftrmqtnysuyRzPbaLcf8vkx4tmjr9UzeJ456qei7JecCzwA3A7wNvV9VXk9wBnF9VfzxlW2vdyjv1l2Te4K5ze2ynN6u+zT39U1Udr6rnuuX3gKPAXuB64P7uZfczegOQtE2d0Tl7Nxf7lcDTwO6qOt59601gd7+lSerTzB+XTXIu8BBwe1W9u/kwqKpq3CF6kg1gY9FCJS1mpimbk+wCHgUeq6q7u3XfB66tquPdef1/VNWvT9nO+p7ksd7nqJN4zr6zzH3OnlGr3AscPRn0ziPALd3yLcDDixYpaTizjMbvA74LvAB82K3+MqPz9geBjwGvAzdV1dtTtrXtu4J17q0mWXZPtt3bscWefabD+L4Y9tUx7KdqMex+gk5qhGGXGmHYpUYYdqkRhl1qhDec3GG2yyjzuDq2+yj9TmbPLjXCsEuNMOxSIwy71AjDLjXCsEuN8NLbGtoul9fmMal2L8sNy55daoRhlxph2KVGGHapEYZdaoSj8afZLqPF6zziPq8h2r7FdhzHnl1qhGGXGmHYpUYYdqkRhl1qhGGXGjHLXG8XJ3kyyUtJXkxyW7f+ziTHkhzqHvuHL3e1kmz56Ht7Xi76/ya1le04m1nmetsD7Kmq55KcBzwL3ADcBLxfVV+feWdrMP3TPLwGrO1k3PRPUz9UU1XHgePd8ntJjgJ7+y1P0tDO6Jw9ySXAlYxmcAW4NcnhJPclOb/v4iT1Z+awJzkXeAi4vareBe4BLgOuYNTz3zXm5zaSHExycPFyJc1rpimbk+wCHgUeq6q7t/j+JcCjVfXJKdvxnH0Tz9k1hLmnbM7oN/Je4OjmoHcDdyfdCBxZtEhJw5llNH4f8F3gBeDDbvWXgS8wOoQv4DXgS91g3qRt7cieXdpOxvXsMx3G98WwS8Ob+zBe0s5g2KVGGHapEYZdaoRhlxph2KVGGHapEYZdaoRhlxph2KVGGHapEYZdaoRhlxph2KVGGHapEYZdaoRhlxph2KVGGHapEYZdaoRhlxph2KVGGHapEYZdaoRhlxoxy1xvv5Dke0meT/Jikj/r1n88ydNJXknyD0nOGb5cSfOapWf/CfDZqvpNRnO7XZfk08DXgG9U1a8B/w18cbAqJS1sathr5P3u6a7uUcBngX/q1t8P3DBEgZL6MdM5e5KzkhwCTgCPA68C71TVB91L3gD2DlKhpF7MFPaq+llVXQFcBFwD/MasO0iykeRgkoPzlSipD2c0Gl9V7wBPAr8FfDTJ2d23LgKOjfmZA1V1dVVdvUihkhYzy2j8Lyf5aLf8i8DngKOMQv+73ctuAR4eqEZJPUhVTX5B8ilGA3BnMXpzeLCq/jzJpcC3gAuA/wR+r6p+MmVbk3cmaWFVla3WTw17nwy7NLxxYfcTdFIjDLvUCMMuNcKwS40w7FIjzp7+kl79GHi9W76we75q1nEq6zjVutXxq+O+sdRLb6fsODm4HT5VZx3W0UodHsZLjTDsUiNWGfYDK9z3ZtZxKus41Y6pY2Xn7JKWy8N4qRErCXuS65J8v7tZ5R2rqKGr47UkLyQ5tMybayS5L8mJJEc2rbsgyeNJXu6+nr+iOu5Mcqxrk0NJ9i+hjouTPJnkpe6mprd165faJhPqWGqbDHaT16pa6oPRn8q+ClwKnAM8D3xi2XV0tbwGXLiC/X4GuAo4smndXwB3dMt3AF9bUR13An+05PbYA1zVLZ8H/AD4xLLbZEIdS20TIMC53fIu4Gng08CDwM3d+r8C/uBMtruKnv0a4JWq+mFV/ZTR38Rfv4I6VqaqngLePm319YzuGwBLuoHnmDqWrqqOV9Vz3fJ7jG6Ospclt8mEOpaqRnq/yesqwr4X+NGm56u8WWUB30nybJKNFdVw0u6qOt4tvwnsXmEttyY53B3mD346sVmSS4ArGfVmK2uT0+qAJbfJEDd5bX2Abl9VXQX8DvCHST6z6oJg9M7O6I1oFe4BLmM0R8Bx4K5l7TjJucBDwO1V9e7m7y2zTbaoY+ltUgvc5HWcVYT9GHDxpudjb1Y5tKo61n09AXybUaOuyltJ9gB0X0+sooiqeqv7RfsQ+GuW1CZJdjEK2N9V1T93q5feJlvVsao26fb9Dmd4k9dxVhH2Z4DLu5HFc4CbgUeWXUSSjyQ57+Qy8HngyOSfGtQjjG7cCSu8gefJcHVuZAltkiTAvcDRqrp707eW2ibj6lh2mwx2k9dljTCeNtq4n9FI56vAn6yohksZXQl4HnhxmXUADzA6HPwfRudeXwR+CXgCeBn4d+CCFdXxt8ALwGFGYduzhDr2MTpEPwwc6h77l90mE+pYapsAn2J0E9fDjN5Y/nTT7+z3gFeAfwR+/ky26yfopEa0PkAnNcOwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUiP8FOCbnU7ho7MUAAAAASUVORK5CYII=\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "171/171 [==============================] - 6s 24ms/step - loss: 25.9518 - accuracy: 0.4929 - val_loss: 11.1240 - val_accuracy: 0.6567\n",
      "Epoch 2/100\n",
      "171/171 [==============================] - 3s 16ms/step - loss: 10.0631 - accuracy: 0.7318 - val_loss: 12.6936 - val_accuracy: 0.7625\n",
      "Epoch 3/100\n",
      "171/171 [==============================] - 3s 15ms/step - loss: 6.8766 - accuracy: 0.8056 - val_loss: 3.7638 - val_accuracy: 0.8889\n",
      "Epoch 4/100\n",
      "171/171 [==============================] - 3s 17ms/step - loss: 5.6883 - accuracy: 0.8496 - val_loss: 12.6956 - val_accuracy: 0.7716\n",
      "Epoch 5/100\n",
      "171/171 [==============================] - 3s 16ms/step - loss: 4.7114 - accuracy: 0.8730 - val_loss: 2.2263 - val_accuracy: 0.8899\n",
      "Epoch 6/100\n",
      "171/171 [==============================] - 3s 17ms/step - loss: 3.7445 - accuracy: 0.8983 - val_loss: 13.0641 - val_accuracy: 0.8538\n",
      "INFO:tensorflow:Assets written to: ./models/classify_shape\\assets\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df_shape.drop(columns=\"0\"), df_shape[\"0\"], test_size=0.16,\n",
    "                                                    random_state=19, stratify=df_shape[\"0\"])\n",
    "\n",
    "x_train, x_test, y_train, y_test = preprocess(x_train, x_test, y_train, y_test, 13)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "model = ConvNetShapeLetter()\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "callback = keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", min_delta=.5, patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit(x=x_train, y=y_train, batch_size=64, epochs=100, validation_data=(x_test, y_test), shuffle=True, callbacks=[callback])\n",
    "model.save(\"./models/classify_shape\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "class ConvNetClassify(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ConvNetClassify, self).__init__()\n",
    "        self.norm = tf.keras.layers.Normalization(axis=None)\n",
    "        self.resnet = keras.applications.ResNet50V2(weights='imagenet', include_top=False,\n",
    "                                                    input_shape=(32, 32, 3))\n",
    "        for layer in self.resnet.layers:\n",
    "            layer.trainable = False\n",
    "        self.model = keras.Sequential([\n",
    "            keras.layers.Flatten(),\n",
    "            keras.layers.Dense(2, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.norm(inputs)\n",
    "        x = self.resnet(x)\n",
    "        return self.model(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n",
      "Epoch 1/100\n",
      "4992/4992 [==============================] - 92s 18ms/step - loss: 0.0104 - accuracy: 0.9996 - val_loss: 4.0890e-05 - val_accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "4992/4992 [==============================] - 92s 18ms/step - loss: 8.0954e-04 - accuracy: 1.0000 - val_loss: 8.1071e-09 - val_accuracy: 1.0000\n",
      "Epoch 3/100\n",
      "4992/4992 [==============================] - 88s 18ms/step - loss: 1.6787e-04 - accuracy: 1.0000 - val_loss: 9.2125e-04 - val_accuracy: 0.9999\n",
      "Epoch 4/100\n",
      "4992/4992 [==============================] - 91s 18ms/step - loss: 2.2252e-04 - accuracy: 1.0000 - val_loss: 1.7128e-08 - val_accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "4992/4992 [==============================] - 91s 18ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.6141e-09 - val_accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "4992/4992 [==============================] - 91s 18ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 6.0760e-08 - val_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: ./models/classify\\assets\n"
     ]
    }
   ],
   "source": [
    "df_shape = pd.read_csv(\"shape_data3.csv\", names=[str(i) for i in range(785)])\n",
    "df_letter = pd.read_csv(\"letter_data.csv\", names=[str(i) for i in range(785)])\n",
    "\n",
    "print(\"DONE\")\n",
    "df_letter[\"0\"] = 0  # 0 is letter, 1 is shape\n",
    "df_shape[\"0\"] = 1\n",
    "\n",
    "df_classify = pd.concat([df_shape, df_letter])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_classify.drop(columns=\"0\"), df_classify[\"0\"], test_size=0.16,\n",
    "                                                    random_state=19, stratify=df_classify[\"0\"])\n",
    "\n",
    "x_train, x_test, y_train, y_test = preprocess(x_train, x_test, y_train, y_test, 2)\n",
    "\n",
    "model = ConvNetClassify()\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "callback = keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", min_delta=.5, patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit(x=x_train, y=y_train, batch_size=64, epochs=100, validation_data=(x_test, y_test), shuffle=True, callbacks=[callback])\n",
    "model.save(\"./models/classify\") # one hot encoding, [1, 0] is letter, [0, 1] is shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ConvNetRotation(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ConvNetRotation, self).__init__()\n",
    "        self.norm = tf.keras.layers.Normalization(axis=None)\n",
    "        self.resnet = keras.applications.ResNet50V2(weights='imagenet', include_top=False,\n",
    "                                                    input_shape=(32, 32, 3))\n",
    "        for layer in self.resnet.layers:\n",
    "            layer.trainable = False\n",
    "        self.model = keras.Sequential([\n",
    "            keras.layers.Flatten(),\n",
    "            keras.layers.Dense(360, activation='relu'),\n",
    "            keras.layers.Dropout(0.2),\n",
    "            keras.layers.Dense(1, activation='tanh')\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.norm(inputs)\n",
    "        x = self.resnet(x)\n",
    "        return self.model(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def angle_difference(x, y):\n",
    "    \"\"\"\n",
    "    Calculate minimum difference between two angles.\n",
    "    \"\"\"\n",
    "    return 180 - abs(abs(x - y) - 180)\n",
    "\n",
    "def angle_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the mean diference between the true angles\n",
    "    and the predicted angles. Each angle is represented\n",
    "    as a binary vector.\n",
    "    \"\"\"\n",
    "    print(y_true, y_pred)\n",
    "    diff = angle_difference(keras.backend.argmax(y_true), keras.backend.argmax(y_pred))\n",
    "    return keras.backend.mean(keras.backend.cast(keras.backend.abs(diff), keras.backend.floatx()))\n",
    "\n",
    "\n",
    "def angle_error_regression(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the mean diference between the true angles\n",
    "    and the predicted angles. Each angle is represented\n",
    "    as a float number between 0 and 1.\n",
    "    \"\"\"\n",
    "    return keras.backend.mean(angle_difference(y_true * 360, y_pred * 360))\n",
    "\n",
    "df_rotation = pd.read_csv(\"rotation.csv\", names=[str(i) for i in range(785)])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_rotation.drop(columns=\"0\"), df_rotation[\"0\"], test_size=0.16,\n",
    "                                                    random_state=19, stratify=df_rotation[\"0\"])\n",
    "\n",
    "x_train, x_test, y_train, y_test = preprocess(x_train, x_test, y_train, y_test, 360, regression=True)\n",
    "\n",
    "model = ConvNetRotation()\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=\"mse\", metrics=[angle_error])\n",
    "\n",
    "callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=.5, patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit(x=x_train, y=y_train, batch_size=64, epochs=100, validation_data=(x_test, y_test), shuffle=True, callbacks=[callback])\n",
    "model.save(\"./models/rotation\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def kmeans( image, n ):                     # Uses k-Means to quantize an image into n colors.\n",
    "    image = cv2.cvtColor( image, cv2.COLOR_BGR2LAB )# The LAB color space is especially useful for this type of system, as it's similar to how a human eye works\n",
    "    w, h, _ = image.shape                           # Gets the shape of the image for the next step\n",
    "    image = image.reshape( ( image.shape[ 0 ] * image.shape[ 1 ], 3 ) ) #k-Means required an image to be one pixel tall for some reason\n",
    "    clt = MiniBatchKMeans( n_clusters = n )         # Creates a new kMeans system. Cluster count tells how many regions of best fit should be made.\n",
    "    labels = clt.fit_predict( image )               # This takes a aet of inputted colors and tries to find clusters of data. Similar to how one might move a '3d cursor' to the center of data on -a 3d mqp\n",
    "    quant = clt.cluster_centers_.astype( \"uint8\" )[ labels ]    #Replaces every pixel with the closewt color k-Means found\n",
    "    quant = quant.reshape( ( h, w, 3 ) )            # Rescales the quantized image back into its original shape. Simply imagine the pixels line wrqpping similar to a wore processor\n",
    "    quant = cv2.cvtColor( quant, cv2.COLOR_LAB2BGR )# Reverts the color as well\n",
    "    #for i in labels: print(labels)\n",
    "    return quant, labels\n",
    "\n",
    "def mask(img, color):\n",
    "    image = img.copy()\n",
    "    for x in range(image.shape[0]):\n",
    "        for y in range(image.shape[0]):\n",
    "            if not np.array_equal(image[x, y], color):\n",
    "                image[x, y] = np.array([0, 0, 0])\n",
    "            else:\n",
    "                image[x, y] = np.array([255, 255, 255])\n",
    "    return image\n",
    "\n",
    "\n",
    "def get_shape_and_image_picture(img: np.array):\n",
    "    new_img = np.reshape(img, (-1, 3))\n",
    "    colors = np.unique(new_img, axis=0)\n",
    "    l = []\n",
    "    for color in colors:\n",
    "        image = mask(img, color)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        d = avg_distance_from_center(image.copy())\n",
    "        l.append([image, d])\n",
    "    l = sorted(l, key=lambda tup: tup[1])\n",
    "    return l[0][0], l[1][0]  # letter image, shape image\n",
    "\n",
    "def avg_distance_from_center(image: np.array):\n",
    "    subs = (np.argwhere(image>0) - np.array([5/2, 5/2]))\n",
    "    out = np.sqrt(np.einsum('ij,ij->i',subs,subs)).mean()\n",
    "    return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def main(img: np.array):\n",
    "    letter_model = keras.models.load_model(\"models/letter_classifier\")\n",
    "    shape_model = keras.models.load_model(\"models/classify_shape\")\n",
    "    shape_labels = [\"circle\", \"semicircle\", \"quartercircle\", \"triangle\",\n",
    "              \"square\", \"rectangle\", \"trapezoid\", \"pentagon\", \"hexagon\",\n",
    "              \"heptagon\", \"octagon\", \"star\", \"cross\"]\n",
    "    alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "    img = cv2.resize(img, (28,28))\n",
    "    img, labels = kmeans(img, 4)\n",
    "    letter_image, shape_image = get_shape_and_image_picture(img)\n",
    "    shape_image = cv2.bitwise_or(letter_image, shape_image)\n",
    "    letter_image = cv2.cvtColor(letter_image, cv2.COLOR_GRAY2RGB)\n",
    "    shape_image = cv2.cvtColor(shape_image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    letter_image = np.reshape(letter_image, (1, 32, 32, 3)).astype('float32')\n",
    "    shape_image = np.reshape(shape_image, (1, 32, 32, 3)).astype('float32')\n",
    "\n",
    "    letter_matrix = letter_model.predict(letter_image)\n",
    "    shape_matrix = shape_model.predict(shape_image)\n",
    "    return alphabet[np.argmax(letter_matrix)], shape_labels[np.argmax(shape_matrix)]  # letter, shape prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [8]\u001B[0m, in \u001B[0;36m<cell line: 10>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     39\u001B[0m cv2\u001B[38;5;241m.\u001B[39mimshow(letter, letter_image)\n\u001B[0;32m     40\u001B[0m cv2\u001B[38;5;241m.\u001B[39mimshow(shape, shape_image)\n\u001B[1;32m---> 41\u001B[0m \u001B[43mcv2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwaitKey\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m     43\u001B[0m letter_image \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mcvtColor(letter_image, cv2\u001B[38;5;241m.\u001B[39mCOLOR_GRAY2RGB)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "letter_model = keras.models.load_model(\"models/letter_classifier\")\n",
    "shape_model = keras.models.load_model(\"models/classify_shape\")\n",
    "shape_labels = [\"circle\", \"semicircle\", \"quartercircle\", \"triangle\",\n",
    "          \"square\", \"rectangle\", \"trapezoid\", \"pentagon\", \"hexagon\",\n",
    "          \"heptagon\", \"octagon\", \"star\", \"cross\"]\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "letter_correct = 0\n",
    "shape_correct = 0\n",
    "both_correct = 0\n",
    "count = 0\n",
    "with open('./data/vals.txt') as f:\n",
    "    for line in f.readlines():\n",
    "        if count >= 100:\n",
    "            break\n",
    "        path, label = line.split(' ')\n",
    "        label = [int(i) for i in label.split(\",\")]\n",
    "        x1, y1, x2, y2, zero = label\n",
    "        img = cv2.imread(f\"./data/{path}\")\n",
    "        img = img[y1:y2, x1:x2]\n",
    "        nothing, shape, letter, rotation = path.split(\"_\")\n",
    "        rotation = rotation.split(\".\")[0]\n",
    "\n",
    "\n",
    "        size=28  # 28x28 image\n",
    "        #plt.imshow(img)\n",
    "        #plt.show()\n",
    "        img = img.copy()\n",
    "        img = cv2.resize(img, (28,28))\n",
    "\n",
    "        img, labels = kmeans(img, 4)\n",
    "        letter_image, shape_image = get_shape_and_image_picture(img)\n",
    "        shape_image = cv2.bitwise_or(letter_image, shape_image)\n",
    "        #letter_image = cv2.resize(letter_image, (300,300))\n",
    "        #shape_image = cv2.resize(shape_image, (300, 300))\n",
    "        #cv2.imshow(letter, letter_image)\n",
    "        #cv2.imshow(shape, shape_image)\n",
    "        #cv2.waitKey(0)\n",
    "        letter_image = cv2.cvtColor(letter_image, cv2.COLOR_GRAY2RGB)\n",
    "        shape_image = cv2.cvtColor(shape_image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        letter_image = np.reshape(letter_image, (1, 32, 32, 3)).astype('float32')\n",
    "        shape_image = np.reshape(shape_image, (1, 32, 32, 3)).astype('float32')\n",
    "\n",
    "        letter_matrix = letter_model.predict(letter_image)\n",
    "        shape_matrix = shape_model.predict(shape_image)\n",
    "\n",
    "        if alphabet[np.argmax(letter_matrix)] == letter:\n",
    "            letter_correct += 1\n",
    "        if shape_labels[np.argmax(shape_matrix)] == shape:\n",
    "            shape_correct += 1\n",
    "        if alphabet[np.argmax(letter_matrix)] == letter and shape_labels[np.argmax(shape_matrix)] == shape:\n",
    "            both_correct += 1\n",
    "print(letter_correct)\n",
    "print(shape_correct)\n",
    "print(count)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}